{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyNSJYgwicqPiUq3iula+gkx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pashok3d/RemarqueGPT/blob/main/RemarqueGPT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -Uqq ipdb\n",
        "import ipdb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d1a1B8Ov0SbJ",
        "outputId": "af8db790-0c27-48b3-c6ad-cd09ebc6ba1e"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.6 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m51.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tqdm -q\n",
        "!pip install wandb -qqq"
      ],
      "metadata": {
        "id": "q58f3NJEEW59"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "wandb.login()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 144
        },
        "id": "HOe1c2RlJpSd",
        "outputId": "c1a18d73-fc97-4556-fe9d-5aff740edf66"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir dataset"
      ],
      "metadata": {
        "id": "VblH5Otza11y"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Building GPT from scratch and training it on all books of Erich Maria Remarque\n",
        "\n",
        "Available tools: python, pytorch\n",
        "\n",
        "Tasks:\n",
        "1. Load data and tokenize to characters\n",
        "2. Implement GPT model using pytorch\n",
        "3. Train and evaluate the model\n",
        "\n",
        "GPT model structure:\n",
        "1. embedding layer\n",
        "2. positional encoding\n",
        "3. blocks\n",
        "    .1 attention\n",
        "    .2 feedforward\n",
        "4. projection\n",
        "\"\"\"\n",
        "\n",
        "import wandb\n",
        "import torch\n",
        "import math\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm import tqdm\n",
        "\n",
        "WINDOW_SIZE = 64\n",
        "BATCH_SIZE = 16\n",
        "EPOCHS = 10\n",
        "LR = 5e-4\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "config = {\n",
        "    \"learning_rate\": LR,\n",
        "    \"epochs\": EPOCHS,\n",
        "    \"batch_size\": BATCH_SIZE,\n",
        "    \"window_size\": WINDOW_SIZE,\n",
        "}"
      ],
      "metadata": {
        "id": "oVQJ4af2hixP"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run = wandb.init(project=\"remark-gpt\", config=config)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "id": "JyE91wOu52hG",
        "outputId": "9671718b-60d5-4e9f-d834-cd702c741ed1"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcrush_tarash\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.1"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20241230_150646-ge9hij0j</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/crush_tarash/remark-gpt/runs/ge9hij0j' target=\"_blank\">zesty-galaxy-12</a></strong> to <a href='https://wandb.ai/crush_tarash/remark-gpt' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/crush_tarash/remark-gpt' target=\"_blank\">https://wandb.ai/crush_tarash/remark-gpt</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/crush_tarash/remark-gpt/runs/ge9hij0j' target=\"_blank\">https://wandb.ai/crush_tarash/remark-gpt/runs/ge9hij0j</a>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dataset\n",
        "with open(\"dataset/The_Dream_Room_1920_AST_978-5-17-071518-3.txt\", \"r\") as f:\n",
        "    lines = f.readlines()\n",
        "\n",
        "text = \"\\n\".join(lines)\n",
        "tokens = sorted(set(text))\n",
        "\n",
        "# Load train dataset\n",
        "with open(\"dataset/The_Dream_Room_1920_AST_978-5-17-071518-3-train.txt\", \"r\") as f:\n",
        "    lines = f.readlines()\n",
        "\n",
        "train_text = \"\\n\".join(lines)\n",
        "\n",
        "# Load dev dataset\n",
        "with open(\"dataset/The_Dream_Room_1920_AST_978-5-17-071518-3-dev.txt\", \"r\") as f:\n",
        "    lines = f.readlines()\n",
        "\n",
        "dev_text = \"\\n\".join(lines)\n",
        "\n",
        "# Load dev dataset\n",
        "with open(\"dataset/The_Dream_Room_1920_AST_978-5-17-071518-3-test.txt\", \"r\") as f:\n",
        "    lines = f.readlines()\n",
        "\n",
        "test_text = \"\\n\".join(lines)\n",
        "\n",
        "id_to_token = {i: token for i, token in enumerate(tokens)}\n",
        "token_to_id = {token: i for i, token in enumerate(tokens)}\n",
        "\n",
        "\n",
        "def tokenize(text) -> list[int]:\n",
        "    return [token_to_id[ch] for ch in text]\n",
        "\n",
        "\n",
        "def decode(token_ids: list[int]) -> str:\n",
        "    return \"\".join([id_to_token[token_id] for token_id in token_ids])\n",
        "\n",
        "\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, text, context_window_size):\n",
        "        self.tokens = tokenize(text)\n",
        "\n",
        "        self.x = []\n",
        "        self.y = []\n",
        "        for i in range(len(self.tokens) - context_window_size):\n",
        "            self.x.append(self.tokens[i : i + context_window_size])\n",
        "            self.y.append(self.tokens[i + 1 : i + context_window_size + 1])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.x)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return torch.tensor(self.x[idx]), torch.tensor(self.y[idx])\n",
        "\n",
        "\n",
        "class GPTBlock(nn.Module):\n",
        "    def __init__(self, embedding_dim: int, max_len: int, dropout: float = 0.1):\n",
        "        # Attention\n",
        "        super().__init__()\n",
        "        self.Q = nn.Linear(embedding_dim, embedding_dim, bias=False)\n",
        "        self.K = nn.Linear(embedding_dim, embedding_dim, bias=False)\n",
        "        self.V = nn.Linear(embedding_dim, embedding_dim, bias=False)\n",
        "        self.kv_softmax = nn.Softmax(dim=-1)\n",
        "        self.attn_dropout = nn.Dropout(dropout)\n",
        "\n",
        "        # Feedforward\n",
        "        self.f1 = nn.Linear(embedding_dim, embedding_dim * 4)\n",
        "        self.f_act = nn.ReLU()\n",
        "        self.f2 = nn.Linear(embedding_dim * 4, embedding_dim)\n",
        "        self.ff_dropout = nn.Dropout(dropout)\n",
        "\n",
        "        self.register_buffer(\"tril\", torch.tril(torch.ones(max_len, max_len)))\n",
        "\n",
        "        self.ln1 = nn.LayerNorm(embedding_dim)\n",
        "        self.ln2 = nn.LayerNorm(embedding_dim)\n",
        "\n",
        "\n",
        "    def forward(self, inputs):\n",
        "\n",
        "        B, T, C = inputs.shape\n",
        "\n",
        "        norm_inputs = self.ln1(inputs)\n",
        "\n",
        "        q = self.Q(norm_inputs)\n",
        "        k = self.K(norm_inputs)\n",
        "        v = self.V(norm_inputs)\n",
        "\n",
        "        attention_weights = (q @ k.transpose(-1, -2)) / math.sqrt(C)  # shape: (B, T, T)\n",
        "        # ipdb.set_trace()\n",
        "        attention_weights_masked = attention_weights.masked_fill(\n",
        "            self.tril[:T, :T] == 0, -torch.inf\n",
        "        )\n",
        "        attention_scores = self.kv_softmax(attention_weights_masked)\n",
        "        attention_scores = self.attn_dropout(attention_scores)\n",
        "\n",
        "        new_v = attention_scores @ v + inputs\n",
        "\n",
        "        x = self.ln2(new_v)\n",
        "        x = self.ff_dropout(self.f2(self.f_act(self.f1(x)))) + x\n",
        "        return x\n",
        "\n",
        "\n",
        "class GPT(nn.Module):\n",
        "    def __init__(\n",
        "        self, vocab_size: int, max_len: int, embedding_dim: int = 16, blocks_num: int = 4\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.emb = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.pos = nn.Embedding(max_len, embedding_dim)\n",
        "        self.blocks = nn.Sequential(\n",
        "            *[GPTBlock(embedding_dim, max_len) for _ in range(blocks_num)]\n",
        "        )\n",
        "        self.proj = nn.Linear(embedding_dim, vocab_size)\n",
        "\n",
        "        self.loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "    def forward(self, inputs, labels=None):\n",
        "        B, T = inputs.shape\n",
        "        embs = self.emb(inputs)\n",
        "        pos_embs = self.pos(torch.arange(T, device=device))  # (T,C)\n",
        "        blocks_output = self.blocks(embs + pos_embs)\n",
        "        logits = self.proj(blocks_output)  # (B,T,vocab_size)\n",
        "        if labels is not None:\n",
        "            loss = self.loss_fn(logits.view(-1, self.vocab_size), labels.view(-1))\n",
        "            return logits, loss\n",
        "        else:\n",
        "            return logits, None\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "1cqziIlWJvDS"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds = TextDataset(train_text, WINDOW_SIZE)\n",
        "train_dataloader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "dev_ds = TextDataset(dev_text, WINDOW_SIZE)\n",
        "dev_dataloader = DataLoader(dev_ds, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "dev_ds = TextDataset(dev_text, WINDOW_SIZE)\n",
        "dev_dataloader = DataLoader(dev_ds, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "test_ds = TextDataset(test_text, WINDOW_SIZE)\n",
        "test_dataloader = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "model = GPT(vocab_size=len(tokens), max_len=WINDOW_SIZE)\n",
        "model.to(device)\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=LR)"
      ],
      "metadata": {
        "id": "pnf4aP9ywkjD"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.watch(model, log_freq=5000)"
      ],
      "metadata": {
        "id": "HOZUqcOnwlIy"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model.train()\n",
        "# epoch_loss = 0\n",
        "# steps_n = 0\n",
        "# with torch.no_grad():\n",
        "#     for batch in tqdm(train_dataloader):\n",
        "#         input, labels = batch[0].to(device), batch[1].to(device)\n",
        "#         output, loss = model(input, labels)\n",
        "#         epoch_loss += loss.item()\n",
        "#         steps_n += 1\n",
        "#     avg_loss = epoch_loss / steps_n\n",
        "# expected_init_loss = -math.log(1 / 74)\n",
        "# print(f\"initial train loss: {avg_loss:.3f}, with expected of {expected_init_loss:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uSeQTdOowrhS",
        "outputId": "93ade3d1-9d1d-4c9b-993f-fa9985bcefeb"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 12836/12836 [00:45<00:00, 283.16it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "initial train loss: 4.459, with expected of 4.304\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rkENd4W1Pcq5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "LR = 0.00001"
      ],
      "metadata": {
        "id": "FMoC1t_sPcoS"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.AdamW(model.parameters(), lr=LR)"
      ],
      "metadata": {
        "id": "Hb4Cg4quPcYG"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(2):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    val_epoch_loss = 0\n",
        "    steps_n = 0\n",
        "    val_steps_n = 0\n",
        "    test_epoch_loss = 0\n",
        "    test_steps_n = 0\n",
        "    for batch in tqdm(train_dataloader):\n",
        "        input, labels = batch[0].to(device), batch[1].to(device)\n",
        "        output, loss = model(input, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "        epoch_loss += loss.item()\n",
        "        steps_n += 1\n",
        "        run.log({\"train_loss\": loss.item()})\n",
        "\n",
        "    avg_loss = epoch_loss / steps_n\n",
        "    print(f\"epoch {epoch} train loss: {avg_loss:.3f}\")\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(dev_dataloader):\n",
        "            input, labels = batch[0].to(device), batch[1].to(device)\n",
        "            output, loss = model(input, labels)\n",
        "            val_epoch_loss += loss.item()\n",
        "            val_steps_n += 1\n",
        "\n",
        "        for batch in tqdm(test_dataloader):\n",
        "            input, labels = batch[0].to(device), batch[1].to(device)\n",
        "            output, loss = model(input, labels)\n",
        "            test_epoch_loss += loss.item()\n",
        "            test_steps_n += 1\n",
        "\n",
        "    avg_val_loss = val_epoch_loss / val_steps_n\n",
        "    avg_test_loss = test_epoch_loss / test_steps_n\n",
        "    print(f\"epoch {epoch} val loss: {avg_val_loss:.3f}\")\n",
        "    print(f\"epoch {epoch} test loss: {avg_test_loss:.3f}\")\n",
        "    run.log({\"epoch_train_loss\": avg_loss, \"epoch_val_loss\": avg_val_loss, \"epoch_test_loss\": avg_test_loss})\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fC3-JgvRwhxz",
        "outputId": "8b5dad32-7c3b-4b29-b5f7-b2e77d3f680d"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 12836/12836 [02:13<00:00, 96.26it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0 train loss: 2.086\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1588/1588 [00:05<00:00, 295.58it/s]\n",
            "100%|██████████| 1618/1618 [00:05<00:00, 296.03it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0 val loss: 2.010\n",
            "epoch 0 test loss: 2.019\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 12836/12836 [02:13<00:00, 96.47it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 1 train loss: 2.083\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1588/1588 [00:05<00:00, 294.44it/s]\n",
            "100%|██████████| 1618/1618 [00:05<00:00, 296.07it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 1 val loss: 2.009\n",
            "epoch 1 test loss: 2.017\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "artifact = wandb.Artifact('model', type='model')\n",
        "artifact.add_file('model/gpt.pt')\n",
        "run.log_artifact(artifact)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e9eYiDoFRIou",
        "outputId": "4419745a-a0f1-4462-9c1b-a2a25f7abaec"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Artifact model>"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), \"model/gpt.pt\")"
      ],
      "metadata": {
        "id": "svBoMscGFBic"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# wandb.finish()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 405
        },
        "id": "RolC0ZBZlolt",
        "outputId": "13dbce10-d87c-4c8d-af98-e846fe9195c8"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch_train_loss</td><td>█▃▃▂▂▁▁▁</td></tr><tr><td>epoch_val_loss</td><td>█▆▅▄▅▂▁▁</td></tr><tr><td>train_loss</td><td>█▄▄▂▄▃▂▃▂▂▂▂▂▂▃▂▂▃▂▂▂▃▂▂▂▂▂▂▁▂▁▁▂▂▂▁▂▁▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch_train_loss</td><td>2.59363</td></tr><tr><td>epoch_val_loss</td><td>2.57107</td></tr><tr><td>train_loss</td><td>2.52198</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">dandy-field-9</strong> at: <a href='https://wandb.ai/crush_tarash/remark-gpt/runs/5dis0n83' target=\"_blank\">https://wandb.ai/crush_tarash/remark-gpt/runs/5dis0n83</a><br> View project at: <a href='https://wandb.ai/crush_tarash/remark-gpt' target=\"_blank\">https://wandb.ai/crush_tarash/remark-gpt</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20241230_123230-5dis0n83/logs</code>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(model, prompt: str, max_tokens: int = 10, temperature: float = 1.0) -> str:\n",
        "   \"\"\"Generate text using the trained GPT model.\"\"\"\n",
        "   model.eval()\n",
        "   context = tokenize(prompt)\n",
        "   generated = list(context)\n",
        "\n",
        "   with torch.no_grad():\n",
        "       for _ in range(max_tokens):\n",
        "           x = torch.tensor(context[-WINDOW_SIZE:]).unsqueeze(0).to(device)\n",
        "           logits, _ = model(x)\n",
        "           logits = logits[0, -1, :] / temperature\n",
        "           probs = torch.softmax(logits, dim=-1)\n",
        "           next_token = torch.multinomial(probs, num_samples=1).item()\n",
        "           generated.append(next_token)\n",
        "           context = generated\n",
        "\n",
        "   return decode(context)"
      ],
      "metadata": {
        "id": "5hLYEahmiyie"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()  # Ensure model is in evaluation mode\n",
        "prompt = \"Привет, любовь моя \"\n",
        "generated_text = generate_text(\n",
        "    model=model,\n",
        "    prompt=prompt,\n",
        "    max_tokens=1000,\n",
        "    temperature=1.0\n",
        ")\n",
        "print(generated_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "19kiEdCTmOQM",
        "outputId": "d5ce2541-cc07-4db9-bc71-b00af0fbe67f"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Привет, любовь моя визние, Фод.\n",
            "\n",
            "– Де вересть обийных обы дыхойство что их оприцамх мыливых Боткрыв промусь пожолму, незра намолья даже знамет от дишией.\n",
            "\n",
            "– О онужды все кой. Постить Тенщиесназыщите тьмом стрить Эрнст убыть ас познажалась села не уловки. Не ту не можный, вот растсяен.\n",
            "\n",
            "– И очесь гро ей. Гкацен все товал. На. К не лушлоенна леречишь скусстно в шелщинул уголкот звт подумиднаясь ее остельнымую, мназывает.\n",
            "\n",
            "– Зечу. Оздно ччто таки вля часть ма не мне жедиловое…\n",
            "\n",
            "Качким се горда…\n",
            "\n",
            "Посольчитесть.\n",
            "\n",
            "Пото несковь зведивать жизнь и он усливпыски.\n",
            "\n",
            "– Оцм кулоподность: «А При замел побирител дерь из умит чень глишую и отповолизвиица на цвей припобил – дешие убыроскался чумоную разможенный но его госвесли он это кудно сказорния.\n",
            "\n",
            "– В и тотвь дружение ренята все тевя дил просинклия на эткс!\n",
            "\n",
            "Гупесни теуда вылской сеашь доржала тразной же чутро головодость И ткаль. Сшорния? Слен взгласло, как опустинчах то сна скорумы. Я, потения, когда Фридам подолжастился. А кото. Всего него убыннтим так не ле больша\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.finish()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 448
        },
        "id": "Pypcq1hkMsc3",
        "outputId": "099f160a-e123-49e6-8db9-5df5cf88b80a"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch_test_loss</td><td>█▅▄▃▃▂▂▂▂▂▂▁▁▁▁</td></tr><tr><td>epoch_train_loss</td><td>█▅▄▃▂▂▂▂▂▂▁▁▁▁▁</td></tr><tr><td>epoch_val_loss</td><td>█▅▄▃▃▂▂▂▂▂▂▁▁▁▁</td></tr><tr><td>train_loss</td><td>█▇▆▄▄▄▄▃▃▂▂▂▃▃▂▄▃▂▃▃▂▃▁▂▃▁▃▂▂▂▂▁▃▂▂▂▂▂▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch_test_loss</td><td>2.01742</td></tr><tr><td>epoch_train_loss</td><td>2.08324</td></tr><tr><td>epoch_val_loss</td><td>2.00906</td></tr><tr><td>train_loss</td><td>1.99332</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">zesty-galaxy-12</strong> at: <a href='https://wandb.ai/crush_tarash/remark-gpt/runs/ge9hij0j' target=\"_blank\">https://wandb.ai/crush_tarash/remark-gpt/runs/ge9hij0j</a><br> View project at: <a href='https://wandb.ai/crush_tarash/remark-gpt' target=\"_blank\">https://wandb.ai/crush_tarash/remark-gpt</a><br>Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20241230_150646-ge9hij0j/logs</code>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uCKlcs_LRvIT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}